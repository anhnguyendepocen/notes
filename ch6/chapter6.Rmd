Chapter 6: Partial pooling and likelihood
========================================

Partial pooling is one of the primary motivations behind conventional hierarchical models. 
Also termed "borrowing information", partial pooling improves estimates of group-level parameters, particularly when we have $>3$ groups and/or varying sample sizes among groups. 
In this chapter, we illustrate partial pooling and link it to prior distributions in a maximum-likelihood context.

#### Learning goals

- motivation for and definition of partial pooling
- simple hierarchical models with likelihood
- hyperparameters
- varying intercepts (NBA freethrow example) with `lme4`
- partial pooling
- clearing up confusion about nestedness
- predictors for multiple levels
- plotting estimates for different levels from lme4 models

Often, data are structured hierarchically. 
For instance, maybe we sample individual animals within sites, with multiple sites. 
Or, perhaps we sequence multiple genes across multiple individuals. 
There may be more than two levels, for instance if we sample parasites of different species within individuals of different species of hosts across multiple sites in a landscape. 
Commonly, sample sizes are not equal across units at various levels. 
The following example demonstrates how to use partial pooling to generate reliable estimates in the context of wildly varying sample sizes. 

## Partial pooling: free throw example

Suppose we are interested in knowing who the best free throw shooter was in the 2014-2015 NBA season. 
We can pull the data from the [web](http://www.basketball-reference.com/leagues/NBA_2015_totals.html), and plot the proportion of free-throws made by player.

```{r}
rawd <- read.csv("leagues_NBA_2015_totals_totals.csv")
rawd[1:6, 1:11]
```

Some players switched teams mid-season, and they appear on separate rows, one for each team they played on. 
We need to aggregate player data across teams, so that we end up with only one row per player. 
This is a perfect opportunity for a `group_by`, `summarize` operation with the `dplyr` package.

```{r, message=FALSE, fig.width=5, fig.height=9, fig.cap="Free throw percentages for NBA players in the 2014-2015 season, sorted from lowest to highest."}
library(dplyr)
library(ggplot2)

# clean the data
d <- rawd %>%
  group_by(Player) %>%
  summarize(ft_made = sum(FT), 
            ft_miss = sum(FTA) - sum(FT),
            ft_shot = sum(FTA), 
            ft_pct = 100 * sum(FT) / sum(FTA)) %>%
  subset(ft_shot != 0) %>%
  arrange(-ft_pct) %>%
  droplevels()
d

ggplot(d, aes(x=ft_pct, y=reorder(Player, -ft_pct))) + 
  theme_minimal() + 
  geom_point(stat='identity', size=.2) + 
  xlab('Free throw %') + 
  geom_text(aes(label=Player), 
            nudge_x=rep(c(5, -5), length.out=nrow(d)), 
            size=1) + 
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```

Wow! 
Looks like we have some really good (100% accuracy) and really bad (0% accuracy) free throw shooters in the NBA. 
We may be interested in is the probability that each player makes a free throw, which is a latent quantity.
We can calculate maximum likelihood estimates for the probability of making a free throw for each player.
We'll assume that the number of free throws made is a binomial random variable, with $p_i$ to be estimated for the $i^{th}$ player, and $k$ equal to the number of free throw attempts, implying

$$y_i \sim Binom(p_i, k_i)$$

```{r}
# fit binomial glm
m <- glm(cbind(ft_made, ft_miss) ~ 0 + Player, 
         family=binomial, data=d)

# store estimated probabilities
probs <- m %>%
  coef() %>%
  plogis() %>%
  round(digits=4) %>% 
  sort(decreasing=TRUE)
```

The maximum likelihood estimates are equal to the proportion of free throws made.

```{r, fig.cap="Plot showing 1:1 correspondence between the empirical proportion of free throws made and the maximum likelihood estimates for making a free throw."}
plot(d$ft_pct, probs, 
     xlab="Empirial proportion FT made", 
     ylab="MLE: Pr(make FT)")
```

But, can we really trust these estimates? 
It seems ridiculous to conclude that a player in the NBA has probability 0 or 1 of making a free throw.
What if we plot the maximum likelihood estimates along with the number of free throw attempts?

```{r, fig.cap="Maximum likelihood estimates along with the sample size for each player. Notice how there is much more variation in the MLEs for players who took few shots."}
ggplot(d, aes(x=ft_shot, y=ft_pct)) + 
  geom_point(alpha=.6) + 
  xlab('Free throw attempts') + 
  ylab('Proportion of free throws made')
```

It looks like the players with the highest and lowest shooting percentages took the fewest shots. 
We should be skeptical of the maximum likelihood estimates for these players, because we are using very little information to inform the estimates. 
One solution is to select some minimum number of shots made, and only believe estimates for players who have made at least that many shots. 
This is what the NBA does, and the cutoff is 125 shots. 

```{r, fig.cap="Plot of the truncated collection of NBA players, restricting consideration to those with 125 or more free throws made as the NBA does. The empirical shooting percentages of these players are considered valid for naming the best shooter(s) in the league."}
d %>%
  filter(ft_made >= 125) %>%
  ggplot(aes(x=ft_shot, y=ft_pct)) + 
  geom_point(alpha=.6) + 
  xlab('Free throw attempts') + 
  ylab('Free throw percentage')
```

This seems somewhat arbitrary - what's special about the number 125? 
Is there a better way to decide which estimates to trust? 

What if instead of tossing out the data from players that haven't made at least 125 shots, we tried to improve those estimates? 
For instance, we might instead pull these estimates towards the average, and place increasingly more trust in proportions from players with more information (shot attempts). 
So, instead of only using information from player $i$ to inform our estimate for $p_i$, we will use that information along with the information on the rest of the players in the league. 
If we have no information on a player, we'll assume that they're no different than the rest of the players in the league, and we will exclusively use information from other players to make our estimate. 
As we obtain more information on a specific player, we can place more weight on that information. 

This is the intuition behind partial pooling, and this is a very Bayesian idea. 
The secret to implementation lies in placing a prior distribution on $p_i$, the probability that player $i$ makes a free throw, so that:

$$y_i \sim Binom(p_i, k_i)$$

and

$$logit(p_i) \sim N(\mu_p, \sigma_p)$$

such that the likelihood to maximize is: 

$$\prod_{i=1}^{n} [y_i \mid p_i] [p_i \mid \mu_p, \sigma_p]$$

where $\prod_{i=1}^{n} [y_i \mid p_i]$ is the binomial likelihood, $[p_i \mid \mu_p, \sigma_p]$ is the prior distribution on the probabilities, $\mu_p$ is the overall league (among player) average (on the logit scale) for the probability of making a free throw, and $\sigma_p$ represents the variability among players in the probability of making a free throw. 
This type of model is sometimes called a varying-intercept or random-intercept model, though we do not recommend using the term "random intercept" ([more discussion of this point here](http://andrewgelman.com/2005/01/25/why_i_dont_use/)). 
Because $\mu_p$ and $\sigma_p$ determine the distribution of the parameter $p$, they are known as **hyperparameters**. 
This model approaches the previous model with no hyperparameters when $\sigma_p$ approaches $\infty$.
A similar strategy would be to place a beta prior directly on $p_i$ rather than placing a normal prior on $logit(p_i)$.
Conceptually, this model assumes that there is a population of parameter values that has some distribution (in this case, a normal distribution), and the parameter for any particular player is drawn from this distribution. 

Notice that this is not a valid Bayesian model because there are not prior distributions for all the parameters. 
Specifically, there is not a prior for the hyperparameters $\mu_p$ and $\sigma_p$. 
Instead, these parameters are treated as fixed but unknown, and maximum likelihood approaches attempt to find the values of the hyperparameters that maximize the likelihood of the data. 

This model is hierarchical in the sense that we have a within player-level model (each player gets their own $p_i$) and an among-player model (with $\sigma_p$ controlling the among-player variation).
In this chapter, we will implement this model in a maximum likelihood framework, and later we will explore similar models from a Bayesian perspective.
One of the best R packages for fitting these types of models is `lme4`, which can be used to fit all kinds of mixed effects models. 

```{r, message=FALSE}
library(lme4)
m2 <- glmer(cbind(ft_made, ft_miss) ~ (1|Player), 
         family=binomial, data=d)
summary(m2)
```

We used the `glmer` function to implement the above model, which allows for a variety of non-normal response variable distributions. 
We obtain an estimate for $\mu_p$ with the "(Intercept)" parameter (it is `r fixef(m2)`) and this is given on the logit scale, which implies an among-player mean probability of `r plogis(fixef(m2))` on the probability scale.
We also see a maximum likelihood estimate for $\sigma_p$ under the random effects section: `r sqrt(unlist(VarCorr(m2)))`.

Let's visualize the new estimates: 

```{r, fig.cap="Plot of our shrunken estimates vs. the naive estimates. Notice that players who took fewer shots had more strongly shrunked estimates."}
# get estimated probabilities for each player from m2
shrunken_probs <- plogis(fixef(m2) + unlist(ranef(m2)))

# match these to the player names, 
# from the row names of the ranef design matrix
shrunken_names <- m2@pp$Zt@Dimnames[[1]] # extracts names from model object

ranef_preds <- data.frame(Player = shrunken_names, 
                          p_shrink = shrunken_probs)

# join the raw data with the model output
joined <- full_join(d, ranef_preds)

# calculate difference between naive & shrunken MLEs
joined$diff <- joined$ft_pct / 100 - joined$p_shrink

# plot naive MLE vs. shrunken MLE
ggplot(joined, aes(x=ft_pct / 100, y=p_shrink, color=ft_shot)) + 
  geom_point(shape=1) + 
  scale_color_gradientn(colours=rainbow(4)) +
  geom_abline(intercept=0, slope=1, linetype='dashed') +
  xlab('Naive MLE') + 
  ylab('Shrunken MLE')
```

```{r, fig.cap="Plot of our shrunken estimates vs. the naive estimates. This time, instead of coloring the points to indicate how many shots a player took, this plot uses facets to represent the log number of shots taken. Notice that as more shots are taken, the estimates converge exponentially to the 1:1 line (which is to say, linear on the log scale)."}
# using facets instead of colors
joined$`log(shots taken)` <- cut(log(joined$ft_shot), 6)
ggplot(joined, aes(x=ft_pct / 100, y=p_shrink)) + 
  geom_abline(intercept=0, slope=1, linetype='dashed', alpha=.7, size=.5) +
  geom_point(shape=1, alpha=.5, size=1) + 
  facet_wrap(~ `log(shots taken)`, labeller = label_both, nrow=2) +
  xlab('Naive MLE') + 
  ylab('Shrunken MLE')
```

```{r, fig.cap="Another way to visualize shrinkage, this time as the difference between the naive and shrunken estimates as a function of sample size."}
# view difference in estimates as a function of freethrows shot
ggplot(joined, aes(x=ft_shot, y=diff)) + 
  geom_point(shape=1) + 
  xlab("Free throw attempts") +
  ylab("Naive MLE - Shrunken MLE")
```

The estimates from the hierarchical model differ from the MLE estimates obtained in our first model. 
In particular, the estimates that are imprecise (e.g., players with few attempted shots) are shrunken towards the grand mean. 
This **shrinkage** is highly desirable, and is consistent with the idea that we have increasing trust in estimates that are informed by more data.

What about the NBA's magic number of 125? 
Do we find that estimates are still undergoing shrinkage beyond this range, or are the empirical free throw percentages reliable if players have made 125 or more shots? 

```{r, fig.cap="Subset of players eligible to be the best in the NBA based on empirical proportions. Notice that some of these estimates are still undergoing shrinkage, although to a lesser degree."}
joined %>%
  filter(ft_made >= 125) %>%
  ggplot(aes(x=ft_shot, y=diff)) + 
    geom_point(shape=1) + 
    xlab("Free throw attempts") +
    ylab("Naive MLE - Shrunken MLE")
```

It looks like there are slight differences between the naive estimates (empirical proportions) and the probabilities that we estimate with partial pooling. 
We might conclude that the 125 shot threshold could give estimates that are reliable to within about 2 percentage points based on the above graph. 

So, which player do we think is the best and the worst? 

```{r}
# range() returns the min and max values
joined %>% filter(p_shrink %in% range(p_shrink))
```

Congrats Steph Curry (for this accomplishment and winning the NBA title), and our condolences to Joey Dorsey, who as of 2015 is playing in the Turkish Basketball League. 

## Partial, complete, and no pooling

Partial pooling is often presented as a compromise between complete pooling (in the above example, combining data from all players and estimating one NBA-wide $p$), and no pooling (using the observed proportion of free-throws made). 
This can be formalized by considering what happens to the parameter-level model in these three cases. 
With no pooling, the among-group (e.g., player) variance parameter approaches infinity, such that $p_i \sim N(\mu_p, \sigma_p): \sigma_p \rightarrow \infty$.
With complete pooling, the among-group variance parameter approaches zero, so that all groups recieve the group-level mean. 
With partial pooling, the estimation of $\sigma_p$ leads to a data-informed amount of shrinkage. 

## Multiple levels, nestedness, and crossedness

In the previous example we had two levels: within and among players. 
Many hierarchical models have more than two levels. 
For instance, at the among-player level, we know that players played on different teams. 
Perhaps there are systematic differences among teams in free-throw shooting ability, for instance because they have a coaching staff that emphasize free-throw shooting, or scouts that recruit players who are good at shooting free-throws. 
We can expand the previous model to include team effects as follows: 

$$y_i \sim Binom(p_i, k_i)$$

$$logit(p_i) = p_0 + \pi_i + \tau_{j[i]}$$

$$\pi_i \sim N(0, \sigma_\pi)$$

$$\tau_j \sim N(0, \sigma_\tau)$$

so that the likelihood to maximize is: 

$$[y\mid p] [p \mid p_0, \pi, \tau] [\pi \mid \sigma_\pi] [\tau \mid \sigma_\tau]$$

Here, $p_0$ is an intercept parameter that represents the mean logit probability of making a free throw across all players and teams. 
Player effects are represented by $\pi_i$, and team effects are represented by $\tau_{j[i]}$, with subscript indexing to represent that player $i$ belongs to team $j$.

Note that not all players play for all teams, that is, players are not "crossed" with teams. 
Most players, specifically those that only played for one team, are nested within teams. 
However, because some players switched teams partway through the season, these players will show up on different teams. 
There is often a lot of confusion around nestedness in these types of models.
We point out here that **nestedness is a feature of the data**, not a modeling decision. 
There are cases when the data are nested but structured poorly so that extra work is required to adequately represent the nestedness of the data. 
For instance, if we had data with measurements from 5 regions, each with 3 sites, and the sites were always labeled 1, 2, or 3, then our data might look like this:

```{r, echo=FALSE}
d <- data.frame(region = rep(1:5, each=3), 
                site = 1:3)
d
```

This would indicate that sites are crossed with region, with each site occurring in each region.
But, this is misleading. 
The observations corresponding to site 1 are actually 5 different sites, occuring in 5 different regions. 
A more accurate data structure would be: 

```{r, echo=FALSE}
d <- data.frame(region = rep(1:5, each = 3), 
                site = 1:15)
d
```

This numbering scheme accurately captures the notion that each site occurs in only one region. 

### Fitting a 3 level model with `lme4`

Returning to the example with player and team effects, both drawn from a prior distribution with hyperparameters $\sigma_\pi$ and $\sigma_\tau$ to be estimated:

```{r, fig.cap="Marginal likelihood profiles for the model hyperparameters."}
# group data at the player level instead of aggregating across teams
d <- rawd %>%
  group_by(Player, Tm, Age, Pos) %>%
  summarize(ft_made = sum(FT), 
            ft_miss = sum(FTA) - sum(FT),
            ft_shot = sum(FTA), 
            ft_pct = sum(FT) / sum(FTA)) %>%
  subset(ft_shot != 0) %>% # removes players with no free throw attempts
  arrange(-ft_pct) %>%
  droplevels()
d

# fit model
m <- glmer(cbind(ft_made, ft_miss) ~ (1|Player) + (1|Tm), 
         family=binomial, data=d)
summary(m)

# profile the likelihood
pr <- profile(m)
confint(pr)

# visualize likelihood profiles
library(lattice)
densityplot(pr)
```

So, it appears that the MLE for the variance attributable to teams is zero. 
Profiling the likelihood, we see that there may be a little bit of variance attributable to teams, but we may be better off discarding the team information and instead including information about the player's positions. 
The logic here is that we expect players to vary in their shooting percentages based on whether they are guards, forwards, centers, etc. 

```{r}
m2 <- glmer(cbind(ft_made, ft_miss) ~ (1|Player) + (1|Pos), 
            family=binomial, data=d)
summary(m2)
AIC(m, m2)
```

So, `m2` seems to be better in terms of AIC, and `m2` will also be more useful for predictive applications. 
For instance, if we wanted to predict the free throw shooting ability of a new player, we could get a more precise estimate with `m2` because we could use information about their position (if it was known). 
In contrast, in model `m`, we would predict the same $p$ regardless of position, because position is not in the model. 

## Level-specific covariates

In hierarchical models, covariates can be included at specific levels. 
For instance, at the player level, we might expect that age has an impact on free throw shooting ability. 
Possibly, players peak at some age, and then start to go downhill as they approach retirement. 
We can represent this with a second degree polynomial effect of age. 
Trying this:

```{r}
m3 <- glmer(cbind(ft_made, ft_miss) ~ Age + I(Age^2) + (1|Player) + (1|Pos), 
            family=binomial, data=d)
```

We get a warning about convergence resulting from numeric issues, with the recommendation to rescale our variables. 
Recall that unscaled continuous covariates tend to cause correlations between the estimates of slopes and intercepts, which can cause major problems for the optimization algorithms that are used to identify maximum likelihood estimates. 
Rescaling age does the trick in this example:

```{r}
d$age <- (d$Age - mean(d$Age)) / sd(d$Age)
m3 <- glmer(cbind(ft_made, ft_miss) ~ age + I(age^2) + (1|Player) + (1|Pos), 
            family=binomial, data=d)
summary(m3)

AIC(m, m2, m3)
```

This model does seem to receive more support than the other two models. 
We can visualise the age result as follows: 

```{r, fig.cap="Plot showing the quadratic effect of age on the logit probability of making a freethrow."}
lo <- 100
new_age <- seq(min(d$age), max(d$age), length.out=lo)
X <- matrix(c(rep(1, 100), new_age, new_age^2), nrow=lo)
logit_p <- X %*% fixef(m3)
scaled_age <- new_age * sd(d$Age) + mean(d$Age) 
plot(scaled_age, plogis(logit_p), type='l', 
     xlab="Age", ylab="p")
```

There are also packages that are designed to aid in visualization of the output of lmer and glmer objects. 
Handy plots here include a sorted caterpillar plot of the random effects:

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=60, fig.cap="Random effects and confidence intervals as generated by the `sjPlot` package. We note here that these confidence intervals are probably unreliable due to restrictive assumptions."}
library(sjPlot)
library(arm)
sjp.glmer(m3, ri.nr = 1, sort = "(Intercept)")
```

```{r, fig.cap="Player position random effects along with Wald confidence intervals."}
sjp.glmer(m3, ri.nr=2, sort = "(Intercept)")
```

These confidence intervals should be taken with a grain of salt, as they are calculated based on a normal approximation with Wald standard errors, which assume a quadratic log-likelihood profile. 
Later, we will get a more reliable estimate of the error for random effects using Bayesian methods. 

We assumed that the random effects are normally distributed on a logit scale. 
This assumption can be checked with a q-q plot: 

```{r, fig.cap="Q-Q plot for the random effects, which are assumed to be distributed normally."}
sjp.glmer(m3, type = "re.qq", facet.grid=F)
```

Last, we should do a sanity check for our estimated probabilities.
One approach is to visually check the estimated probabilities vs. the naive empirical proportions.

```{r, fig.cap="Shrinkage plot with the updated model."}
d$estimated_p <- fitted(m3)
d$diff <- d$ft_pct - d$estimated_p
ggplot(d, aes(x=ft_shot, y=diff)) + 
  geom_point(shape=1) + 
  xlab("Free throw attempts") +
  ylab("Naive MLE - Shrunken MLE")
```

```{r, fig.cap="An alternative shrinkage plot to compare naive and shrunken estimates."}
ggplot(d) + 
  geom_segment(aes(x="Naive MLE", xend="Shrunken MLE", 
                   y=ft_pct, yend=estimated_p, group=Player), 
               alpha=.3) + 
  xlab('Estimate') + 
  ylab('Pr(make freethrow)')
```

```{r, fig.cap="Final sanity check to ensure that the estimated probabilities from the final model are reasonable."}
ggplot(d, aes(x=ft_shot, y=estimated_p)) + 
  geom_point(shape=1) + 
  xlab("Free throw attempts") +
  ylab("Shrunken MLE")
```

Who does this model identify as the worst?

```{r}
d[which.min(d$estimated_p), 
  c("Player", "Age", "Pos", "ft_shot", "ft_pct", "estimated_p")]
```

Which player might be best?

```{r}
d[which.max(d$estimated_p), 
  c("Player", "Age", "Pos", "ft_shot","ft_pct", "estimated_p")]
```

```{r, echo=FALSE, results='hide', message=FALSE}
# detach arm
detach("package:arm", unload=TRUE)
```

Shrinkage is one of the primary advantageous of hierarchical models, because it makes better use of all of the available information, and can improve estimates for groups with less information. 
This particular example may seem somewhat esoteric, but as we will see later, it is very common in real datasets to have wildly varying amounts of information for groups at different levels. 

## Further reading

Gelman and Hill. 2009. *Data analysis using regression and multilevel/hierarchical models*. Chapter 11, 12.

Gelman et al. 2014. *Bayesian data analysis, 3rd edition*. Chapter 5. 

Efron, Bradley, and Carl N. Morris. Stein's paradox in statistics. WH Freeman, 1977.

Gelman, Andrew, Jennifer Hill, and Masanao Yajima. "Why we (usually) don't have to worry about multiple comparisons." Journal of Research on Educational Effectiveness 5.2 (2012): 189-211.
