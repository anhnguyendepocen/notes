Chapter 4: Poisson models
==============

## Big picture

The Poisson distribution is often used for discrete counts, as it has integer support. 
In this chapter, we show how to extend the methods of Chapter 3 to models that use Poisson likelihoods, and we point out connections to Poisson-family generalized linear models. 
Also, we make use of link functions as maps between constrained and unconstrained spaces, and show how to include overdispersion in Poisson models, which is often necessary in real-world applications.

#### Learning goals

- parameters and behavior of the Poisson distribution
- log link as a map
- effects sizes in Poisson models
- dependence of mean and variance in non-Gaussian models 
- overdispersion: Poisson and negative-binomial
- implementation with `glm` and Stan
- graphical displays 
- model checking
- simulation of data & parameter recovery

## Poisson generalized linear models

We have covered general linear models, which by definition assume normally distributed response variables, and in this chapter we cover our first generalized linear model. 
Generalized linear models (glms) allow for other response distributions, including the Poisson distribution. 
This is admittedly a subtle distinction. 

Poisson glms are defined as follows:

$$y \sim Poisson(\lambda)$$

$$log(\lambda) = X \beta$$

Where the log-link is used to map the Poisson distribution's only parameter $\lambda$ from it's constrained space $[0, \infty)$ to the unconstrained space $(-\infty, \infty)$, and $X$ is a design matrix as before. 
Recall that $\lambda$ is the mean and variance of the Poisson distribution, in contrast for example to the Gaussian distribution which has separate parameters for the first and second moments.

Sometimes, it's desirable to include an offset. 
For instance, perhaps $y$ is the number of events in some period of time, or in some area, volume, etc., and each observation has different time intervals, areas, or volumes sampled. 
A simple example would be counting the number of zooplankton in water samples that vary in their volume.
Then, we might be interested in the rate of events or event density per unit (time or meters square), e.g., $\dfrac{\lambda}{t}$, which is in units events per time.

$$log(\lambda / t) = X \beta$$

$$log(\lambda) - log(t) = X \beta$$

$$log(\lambda) = X \beta + log(t)$$

So that now, we can still rely on $y \sim Poisson(\lambda)$, even with varying $t$, where $y$ is the number of events counted.
Here, we are assuming that the time period sampled is known without error.

The class of models that utilize the Poisson distribution is far broader than what we can do with glms, but glms provide a common and simple starting point. 

## Simulation and estimation

We will demonstrate with a Poisson regression, but maintain the generality of our notation by continuing to make use of the design matrix $X$, so that the applicability to ANOVA, ANCOVA, etc. analogues with Poisson responses remains apparent. 

```{r}
n <- 50
x <- runif(n, -1, 1)
X <- matrix(c(rep(1, n), x), ncol=2)
beta <- c(.5, -1)
lambda <- exp(X %*% beta)
y <- rpois(n, lambda)
```

### Estimation with `glm`

We can obtain maximum likelihood estimates for Poisson glm coefficients, along with frequentist p-values and confidence intervals with the `glm` function. 

```{r}
m <- glm(y ~ x, family=poisson)
summary(m)
confint(m)
```

Plotting the line of best fit with the data: 

```{r, fig.cap="Line of best fit from the Poisson glm along with the raw data."}
plot(x, y)

xnew <- seq(min(x), max(x), length.out = 100)
lines(xnew, exp(coef(m)[1] + coef(m)[2] * xnew))
```

### Estimation with Stan

Even though Poisson glms are often taught after Gaussian glms, they are in a way simpler, requiring one less parameter. 
While the normal distribution has separate parameters for the mean $\mu$ and variance $\sigma^2$, with the Poisson distribution, the parameter $\lambda$ is the mean *and* the variance. 
Here is a Stan file `poisson_glm.stan` that can be used generally to fit any Poisson glm without an offset. 

```
// Stan code: Poisson GLM with no offset
data {
  int<lower = 1> n; // sample size
  int<lower = 1> p; // number of coefficients
  matrix[n, p] X;
  int<lower = 0> y[n];
}

parameters {
  vector[p] beta;
}

model {
  beta ~ normal(0, 3);
  y ~ poisson_log(X * beta);
}
```

Fitting the model is the same as before:

```{r, message=FALSE, warnings=FALSE, results='hide'}
library(rstan)
stan_d <- list(n = nrow(X), p = ncol(X), X = X, y = y)
out <- stan('poisson_glm.stan', data = stan_d)
```

As always, it behooves us evaluate convergence and see whether there may be issues with the MCMC algorithm. 

```{r, fig.cap="Traceplot of Markov chains for the Poisson glm."}
out
traceplot(out)
```

```{r, fig.cap="Default `stanfit` plot output for the Poisson glm."}
plot(out)
```

```{r, fig.cap="A pairs plot for the posterior of the Poisson glm."}
pairs(out)
```

Let's plot the lines of best fit from the posterior distribution:

```{r, fig.cap="Raw data along with lines of best fit for the Poisson glm."}
# load scales package for alpha() transparency function
library(scales) 

# extract posterior samples into a list containing arrays
post <- rstan::extract(out)

# plot raw data
plot(x, y)

# add a faint line for each posterior draw
n_iter <- length(post$lp__)
for (i in 1:n_iter){
  lines(xnew, exp(post$beta[i, 1] + post$beta[i, 2] * xnew), 
        col = alpha('purple3', .01))
}

# add the points again as solid circles
points(x, y, pch=19)
```

At times, people get confused about why the fits from Poisson glms are not linear, even though we didn't include any polynomial terms. 
The model is linear on the link-scale only, so that if we were to plot x vs. $log(\lambda)$, we would see a straight line. 
There is a lot of good material in Gelman and Hill, Chapter 6 to help with the interpretation of glm parameter estimates. 
Visualization is an excellent step towards interpreting model output. 

## Overdispersion

The variance of counts in nature tends to be greater than the mean. 
This can be considered to be extra-Poisson variance. 
Various strategies exist to account for overdispersion. 
Here we cover the inclusion of lognormal random effects, and introduce the negative binomial distribution which arises as a Poisson distribution with a gamma-distributed $\lambda$. 

### Checking for overdispersion

How do you know whether the variance in your data exceeds the variance you would expect in the absence of overdispersion? 
One general strategy that falls under the class of model diagnostics known as **posterior predictive checks** does the following:

1. For each posterior draw, simulate a new vector of observations. 
2. For each simulated vector of observations, calculate some test statistic of interest. 
3. Compare the distribution of simulated test statistics to the empirical test statistic.

In this case, it is reasonable to choose the variance of $y$ as the test statistic. 

```{r, fig.cap="Histogram of simulated variances from the posterior predictive distribution along with the observed variance, comprising a posterior predictive check for the variance of $y$."}
# simulate new observations and store variances
y_new <- array(dim=c(n_iter, n))
var_new <- rep(NA, n_iter)
for (i in 1:n_iter){
  y_new[i, ] <- rpois(n, exp(X %*% post$beta[i, ]))
  var_new[i] <- var(y_new[i, ])
}

# compare distribution of simulated values to the empirical values
hist(var_new, breaks=40, xlab='Var(y)',
     main='Posterior predictive check \n for overdispersion')
abline(v = var(y), col=2, lwd=3)
```

In this case, we can say that the variance in y shown by the red line is perfectly compatible with the model, which makes sense because we generated the data from a Poisson distribution. 

Here's a real world application with overdispersion.
The `vegan` package has a dataset of the number of trees in a bunch of 1-hectare plots on Barro Colorado Island in Panama. 
Let's look at the mean and variance for the counts of each species:

```{r, message=FALSE, fig.cap="Sample mean and variances for abundance on Barro Colorado Island, with each point representing a different species. If the mean equals the variance, as the Poisson distribution assumes, points will fall on the dashed line."}
library(vegan)
library(dplyr)
data(BCI)

# coerce into long form
d <- stack(BCI)
str(d)

# calculate means and variances
summ_d <- d %>%
  group_by(ind) %>%
  summarize(lmean=log(mean(values)), 
            lvar=log(var(values)))

# plot the log-mean vs. the log-variance
ggplot(summ_d, aes(x=lmean, y=lvar)) + 
  geom_point() + 
  geom_abline(intercept=0, slope=1, linetype='dashed') + 
  xlab("log mean") + 
  ylab("log variance")
```

Darn.
Looks like the variance exceeds the mean for most of these species. 
But, we'll need to do a posterior predictive check in order to more formally evaluate whether the variance is consistent with the assumption that the counts are Poisson distributed.
Let's put together a simple model for the abundance of the species *Trichilia pallida*, where we seek to estimate the mean density for the island based on the sampled plots. 
This is a fairly simple model - there are no covariates, and the design matrix is just a vector of ones, similar to the "model of the mean" that we encountered in chapter 1, but with a Poisson distributed response. 

```{r, message=FALSE, warnings=FALSE, results='hide', fig.cap="Sorted abundance values for *Trichilia pallida*, with each point representing the abundance of the species in a sample plot."}
# subset data
species_d <- subset(d, ind == 'Trichilia.pallida')

# visualize abundance data
plot(sort(species_d$values), ylab="Sorted abundance")

# collect data for estimation
stan_d <- list(n = nrow(species_d), p = 1, 
               X = matrix(1, nrow = nrow(species_d)), 
               y = species_d$values)

# estimate parameters
out <- stan('poisson_glm.stan', data = stan_d)
```

Assessing convergence: 

```{r, fig.cap="Traceplot of the Markov chain for the single species Poisson glm."}
out
traceplot(out)
```

We will use a posterior predictive check for the variance of $y$. 
Again, the goal here is to simulate new data from the posterior, and see whether the data generated from our model are consistent with the observed data. 
Specifically, we want to know whether the variance in the data exceeds the variance that our model would predict.

```{r, fig.cap="Posterior predictive check for the variance of plot abundance from the single species Poisson glm. The histogram represents the posterior predictive variance, and the red line represents the variance in the data."}
# extract posterior draws into a list of arrays
post <- rstan::extract(out)

# simulate new observations and store variances for each posterior draw
y_new <- array(dim=c(n_iter, n))
var_new <- rep(NA, n_iter)
for (i in 1:n_iter){
  y_new[i, ] <- rpois(n, exp(post$beta[i, 1]))
  var_new[i] <- var(y_new[i, ])
}

# compare distribution of simulated values to the empirical values
hist(var_new, breaks=40, xlab='Var(y)', xlim=c(0, 7),
     main='Posterior predictive check \n for overdispersion')
# add red line for empirical variance
abline(v = var(stan_d$y), col=2, lwd=3)
```

As we can see, the observed variance is more than twice the expected variance under a Poisson model. 
Thus, we would conclude that we have strong evidence for overdispersion.

### Lognormal overdispersion

We will expand our model so that we can include some overdispersion. 
First, we will allow for additional plot-level variance by adding a term to our linear predictor:

$$y_i \sim Poisson(\lambda_i)$$

$$log(\lambda_i) = X_i' \beta + \epsilon_i$$

$$\epsilon_i \sim Normal(0, \sigma)$$

$$\sigma \sim halfCauchy(0, 2)$$

Here, each observation $y_i$ has an associated parameter $\epsilon_i$ that has a normal prior, with a variance hyperparameter that determines how much extra-Poisson variance we have. 
If there is no overdispersion, then the posterior probability mass for $\sigma$ should be near zero. 
We can also represent this model with a directed acyclic graph.

![Directed acyclic graph of the overdispersed Poisson model.](ch4/dag.pdf)

Our Stan model `poisson_od.stan` might be:

```
// Stan code for the overdispersed Poisson model
data {
  int<lower = 1> n; // sample size
  int<lower = 1> p; // number of coefficients
  matrix[n, p] X;
  int<lower = 0> y[n];
}

parameters {
  vector[p] beta;
  vector[n] epsilon;
  real<lower = 0> sigma;
}

model {
  // priors
  beta ~ normal(0, 3);
  sigma ~ normal(0, 2);
  epsilon ~ normal(0, sigma);

  // likelihood
  y ~ poisson_log(X * beta + epsilon);
}
```

Our data haven't changed, but we do need to specify the path to this updated model:

```{r, message=FALSE, warnings=FALSE, results='hide'}
od_out <- stan('poisson_od.stan', data = stan_d)
```

```{r, fig.cap="Traceplot of the Markov chain for the overdispersed Poisson model."}
print(od_out, pars=c('sigma', 'beta', 'lp__'))
traceplot(od_out, pars=c('sigma', 'beta'))
```

Using an updated posterior predictive check for the variance of $y$:

```{r, fig.cap="Updated posterior predictive check for the variance of $y$ with the overdispersed Poisson model."}
post <- rstan::extract(od_out)

# simulate new observations and store variances
y_new <- array(dim=c(n_iter, n))
var_new <- rep(NA, n_iter)
for (i in 1:n_iter){
  y_new[i, ] <- rpois(n, exp(post$beta[i, 1] + post$epsilon[i, ]))
  var_new[i] <- var(y_new[i, ])
}

# compare distribution of simulated values to the empirical values
hist(var_new, breaks=40, xlab='Var(y)', 
     main='Posterior predictive check \n for overdispersion')
abline(v = var(stan_d$y), col=2, lwd=3)
```

That looks much better. 
The variance that we get from our replicated simulations is entirely consistent with the variance that we actually observed. 

### Poisson-gamma overdispersion and the negative binomial

The negative binomial distribution can be thought of as a Poisson distribution with a gamma prior on $\lambda$. 

$$y_i \sim Poisson(\lambda_i)$$

$$\lambda_i \sim Gamma(\alpha, \beta)$$

To give a sense of what this looks like, here are some gamma distributions with varying parameters:

```{r, fig.cap="Probability densities of Gamma distributions with different parameters."}
alpha <- c(.1, 1, 10)
beta <- c(.1, 1, 10)
g <- expand.grid(alpha = alpha, beta = beta)
x <- seq(0, 20, .1)
par(mfrow=c(3, 3))
for (i in 1:9){
  plot(x, dgamma(x, g$alpha[i], g$beta[i]), type='l', 
       ylab='[x]')
}
```

We could implement this directly in Stan, but it is easier to use the built-in and optimized negative binomial distribution function, which is parameterized in terms of its mean $\mu$ and precision $\phi$:

$$y \sim NegBinom(\mu, \phi)$$

$$log(\mu) = X \beta$$

$$\phi \sim halfCauchy(0, 2)$$

```
data {
  int<lower = 1> n; // sample size
  int<lower = 1> p; // number of coefficients
  matrix[n, p] X;
  int<lower = 0> y[n];
}

parameters {
  vector[p] beta;
  real<lower=0> phi;
}

model {
  beta ~ normal(0, 3);
  phi ~ cauchy(0, 5);
  y ~ neg_binomial_2_log(X * beta, phi);
}
```

With this `nb_glm.stan` file, we can fit the model as before:

```{r, message=FALSE, warnings=FALSE, results='hide'}
nb_out <- stan('nb_glm.stan', data = stan_d)
```

```{r, fig.cap="Traceplot of the Markov chains for the negative binomial model. "}
nb_out
traceplot(nb_out)
```

Conducting our posterior predictive check:

```{r, fig.cap="Posterior predictive check for the variance in $y$ with the negative binomial likelihood."}
post <- rstan::extract(nb_out)

# simulate new observations and store variances
y_new <- array(dim=c(n_iter, n))
var_new <- rep(NA, n_iter)
for (i in 1:n_iter){
  y_new[i, ] <- rnbinom(n, mu = exp(post$beta[i, 1]), size = post$phi[i])
  var_new[i] <- var(y_new[i, ])
}

# compare distribution of simulated values to the empirical values
par(mfrow=c(1, 1))
hist(var_new, breaks=40, xlab='Var(y)', 
     main='Posterior predictive check \n for overdispersion')
abline(v = var(stan_d$y), col=2, lwd=3)
```

Again, the replicated datasets that are simulated from the posterior predictive distribution now have variance consistent with the observed data. 

## Further reading

Gelman and Hill. 2009. *Data analysis using regression and multilevel/hierarchical models*. Chapter 6.

Gelman et al. 2014. *Bayesian data analysis. Third edition*. Chapter 16.
